# InteliDoc RAG Pipeline

A production-ready Retrieval-Augmented Generation (RAG) pipeline with GPU-accelerated document processing using Docling, vector storage in PostgreSQL (pgvector), and Ollama for local LLM inference.

## ğŸš€ Features

- **ğŸ“„ Document Processing**: Upload and process PDFs, DOCX, HTML, Markdown, and more
- **ğŸ” Vector Search**: Fast similarity search using PostgreSQL pgvector with HNSW indexes
- **ğŸ¤– Local LLM**: Fully local AI responses using Ollama (no API keys needed)
- **âš¡ GPU Accelerated**: Document parsing and embedding generation leverage NVIDIA GPUs
- **ğŸ³ Docker Ready**: Complete Docker Compose setup with GPU support

## ğŸ“‹ Prerequisites

- Docker and Docker Compose
- NVIDIA GPU with CUDA support
- NVIDIA Container Toolkit ([Installation Guide](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html))

Verify GPU support:
```bash
docker run --rm --gpus all nvidia/cuda:12.2-base-ubuntu22.04 nvidia-smi
```

## ğŸ› ï¸ Quick Start

### 1. Clone and Configure

```bash
cd intelidoc

# Copy environment template
cp .env.example .env

# Edit .env if needed (defaults work out of the box)
```

### 2. Start Services

```bash
# Build and start all services
docker compose up -d --build

# View logs
docker compose logs -f
```

### 3. Pull LLM Model

After Ollama starts, pull the default model:
```bash
docker exec intelidoc-ollama ollama pull llama3.2
```

### 4. Access the API

- **API Documentation**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/health

## ğŸ“– API Usage

### Upload a Document

```bash
curl -X POST "http://localhost:8000/documents/upload" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@your_document.pdf"
```

### Check Document Status

```bash
curl "http://localhost:8000/documents"
```

### Query Your Documents

```bash
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is the main topic of the document?",
    "top_k": 5
  }'
```

### Vector Search Only

```bash
curl -X POST "http://localhost:8000/search" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "specific topic to search",
    "top_k": 10
  }'
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Docker Network                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Ollama     â”‚    â”‚   Docling    â”‚    â”‚   PostgreSQL     â”‚  â”‚
â”‚  â”‚   (LLM)      â”‚    â”‚   (Parser)   â”‚    â”‚   + pgvector     â”‚  â”‚
â”‚  â”‚   :11434     â”‚    â”‚   :8001      â”‚    â”‚   :5432          â”‚  â”‚
â”‚  â”‚   [GPU]      â”‚    â”‚   [GPU]      â”‚    â”‚                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚          â–²                  â–²                     â–²             â”‚
â”‚          â”‚                  â”‚                     â”‚             â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                             â”‚                                    â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚                    â”‚   RAG API       â”‚                          â”‚
â”‚                    â”‚   (FastAPI)     â”‚                          â”‚
â”‚                    â”‚   :8000         â”‚                          â”‚
â”‚                    â”‚   [GPU]         â”‚                          â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                             â–²                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                         HTTP/REST
                              â”‚
                        â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
                        â”‚   User    â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
intelidoc/
â”œâ”€â”€ docker-compose.yml      # Service orchestration
â”œâ”€â”€ Dockerfile.api          # RAG API service image
â”œâ”€â”€ Dockerfile.docling      # Document processing image
â”œâ”€â”€ init.sql                # PostgreSQL initialization
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ .env.example            # Environment template
â”œâ”€â”€ app/                    # Main application
â”‚   â”œâ”€â”€ main.py             # FastAPI entry point
â”‚   â”œâ”€â”€ config.py           # Configuration
â”‚   â”œâ”€â”€ database.py         # Database connection
â”‚   â”œâ”€â”€ models.py           # SQLAlchemy models
â”‚   â”œâ”€â”€ schemas.py          # Pydantic schemas
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ documents.py    # Document endpoints
â”‚   â”‚   â””â”€â”€ query.py        # Query endpoints
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ docling_client.py  # Docling client
â”‚       â”œâ”€â”€ embeddings.py      # Embedding service
â”‚       â”œâ”€â”€ vector_store.py    # Vector operations
â”‚       â””â”€â”€ rag_chain.py       # RAG implementation
â””â”€â”€ docling_service/        # Document processing microservice
    â”œâ”€â”€ main.py
    â””â”€â”€ requirements.txt
```

## âš™ï¸ Configuration

| Variable | Default | Description |
|----------|---------|-------------|
| `POSTGRES_USER` | intelidoc | Database user |
| `POSTGRES_PASSWORD` | intelidoc_secret | Database password |
| `POSTGRES_DB` | intelidoc | Database name |
| `LLM_MODEL` | llama3.2 | Ollama model to use |
| `EMBEDDING_MODEL` | sentence-transformers/all-MiniLM-L6-v2 | Embedding model |
| `CHUNK_SIZE` | 500 | Text chunk size |
| `CHUNK_OVERLAP` | 50 | Overlap between chunks |
| `TOP_K_RESULTS` | 5 | Default number of search results |

## ğŸ”§ Development

### Local Development (without Docker)

```bash
# Install dependencies
pip install -r requirements.txt

# Start PostgreSQL and Ollama locally
# ...

# Run the API
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### Rebuild After Changes

```bash
docker-compose build --no-cache api
docker-compose up -d api
```

## ğŸ› Troubleshooting

### GPU Not Detected

```bash
# Verify NVIDIA Container Toolkit
nvidia-ctk --version

# Check Docker GPU support
docker info | grep -i gpu
```

### Model Not Found

```bash
# Pull the required model
docker exec intelidoc-ollama ollama pull llama3.2

# List available models
docker exec intelidoc-ollama ollama list
```

### Database Connection Issues

```bash
# Check PostgreSQL logs
docker-compose logs postgres

# Verify pgvector extension
docker exec intelidoc-postgres psql -U intelidoc -c "SELECT * FROM pg_extension WHERE extname = 'vector';"
```

## ğŸ“„ License

MIT License
